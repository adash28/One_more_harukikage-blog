---
title: "一些探索controlnet的心得"
date: 2026-02-24
draft: false
weight: 202
tags: ["AIGC", "comfyui", "Image Generation"]
categories: ["AIGC Works"]
---

# controlnet与火柴人

![controlnet部分节点连线](img/controlnet.png)

如图，是controlnet的节点连线。


## DWPose Estimator



DWPose Estimator是用来估计人物的姿态的，会把输入的真实人物图转化为火柴人的骨架图。这个节点起的作用，也可采用其他节点估计脸部和身体姿态，后续有兴趣钻研后再补充。

机理：可看作两步走。第一步目标检测：它使用一个独立的目标检测模型（如 YOLO ）来扫描整个图像，并为每个检测到的人绘制一个边界框（Bounding Box） 。这一步至关重要，因为它将后续更精细、计算量也更大的姿态估计任务，限定在了几个很小的区域内，极大地提升了效率和准确性；第二步关键点估计 (Keypoint Estimation)： 在上一步框定的人体区域内，精确地定位出所有身体关节（如头、肩、肘、膝、手腕等）的二维坐标。

注意：也正因它的目标检测模型，它只能识别真实人物，而且不能识别物品。


### 参数详解（以下由Gemini提供解释，不担保完全正确,后续亦如此）

- image (输入) ：需要提取姿态的原始图像。

- detect_hand / detect_body / detect_face ：

  - 功能： 分别控制是否要检测手部、身体和面部的关键点。
  - enable (启用) ：意味着 DWPose 会额外运行专门的手部/身体检测模型，输出更精细的骨架（例如，手部会包含多少个关节点）。这对于生成准确的手部姿势至关重要。
  - disable (禁用) ：跳过该部分的检测，以节省计算资源。

- resolution (分辨率) ：

  - 功能： 在进行姿态估计前，输入图像会被调整到的目标分辨率。
  - 作用： 更高的分辨率能让模型看清更多细节（特别是对于画面中的小人物或复杂的手部动作），从而提升检测精度。但**<span class="fw-bold">同时也会增加显存消耗和处理时间</span>**（处理时间本身不长，而且只用训练一次就可以了）。 1536 是一个相对较高的设置，适合处理高清或细节丰富的图像。(需要根据实际情况调整，gemini说至少要1024以上)

 注意： KSampler 接收的是骨架图，ControlNet 会自动将这张骨架图缩放到与潜空间（Latent Space）匹配的尺寸，故不必担心高分辨率对后续采样的影响。（实际上，我用4032分辨率在一些姿势上取得了比更低分辨率更好的效果）

- bbox_detector (边界框检测器) ：

  - 功能： 指定第一步哪个目标检测模型。
  - yolox_l.onnx ：表示使用的是 YOLO-X (Large) 模型。YOLO 是目前最先进、速度最快的目标检测框架之一。 .onnx 是一种跨平台的标准模型格式。

- dw-ll_ucoco_384_bs5.torchscript.pt ：

  - 功能： 指定在第二步使用的核心姿态估计模型。
  - 命名解析：
  - dw : DWPose 模型。
  - ll : 很可能指代模型尺寸（如 Large/Extra Large）。
  - ucoco : 表示模型是在 UCOCO 数据集上训练的，这是一个包含大量遮挡和罕见姿势的增强版 COCO 数据集。
  - 384 : 模型训练时使用的输入图像尺寸是 384x384 像素。
  - .torchscript.pt : 这是一个经过优化的 PyTorch 模型文件格式。

- scale_stick_for_xinsr_cn ：

  - 功能： 一个兼容性开关，用于适配特定版本的 ControlNet。
  - 作用： 某些非官方或早期版本的 ControlNet 在训练时，其骨架图的肢体比例可能与 DWPose 生成的默认比例有微小差异。开启此选项会将骨架图缩放到与那个特定模型（ xinsr_cn ）更匹配的比例，以获得更好的控制效果。此处为 disable ，表示正在使用标准比例。

## ControlNet

1. 核心思想：权重复制与“零卷积”
- 权重复制： 在加载时，ControlNet 会完整地复制一份 Base Model（如 SDXL UNet）的神经网络权重，我们称之为“可训练副本”。而原始的 Base Model 则被“冻结”，其权重在整个生成过程中保持不变。
- “零卷积” (Zero Convolution)： 这是 ControlNet 的精髓。在“可训练副本”和“冻结副本”之间，有一系列特殊的 1x1 卷积层。这些层在初始化时，其权重和偏置全部为零。

2. 训练与推理过程
- 训练时：
  
  1. 输入一张图片和对应的条件图（如姿态骨架）。
  2. “可训练副本”接收条件图，并尝试生成与原图匹配的图像。
  3. 通过反向传播，只有“零卷积”层和“可训练副本”的权重会被更新。
  - 结果： ControlNet 学会了如何将“姿态骨架”这种抽象信息，转换为能被 UNet 理解的语义特征。

- 推理时（在 ComfyUI 中）：
  
  1. 在 KSampler 的每一步，潜空间噪声会同时流经“冻结副本”和“可训练副本”。
  2. “可训练副本”根据你输入的骨架图，输出一个修正信号 。
  3. 这个修正信号通过“零卷积”层，被加到“冻结副本”对应层的输出上。
  - 效果： 这相当于在 Base Model 每一次去噪决策时，都强行注入一个“方向盘”，告诉它：“嘿，无论你要画什么，都必须先满足这个姿势！”

## 其他收获

1. ControlNet 的工作机理，是基于对 UNet架构的深度耦合.UNet的内部是由一系列卷积块（Convolutional Blocks）和下采样/上采样层构成的；Transformer Base Model： 它的内部是由一系列自注意力块（Self-Attention Blocks）构成的。故不适用处理Transformer Base Model。（或许以后会有处理Transformer Base Model的版本）

2. 除了处理姿势，controlnet还可以处理深度图。深度图是一张灰度图像，其每个像素的亮度代表了原始场景中该点距离摄像机的远近。它通过专门的深度估计算法（如 MiDaS , LeReS , 或最新的 DepthAnything ）生成。

3. 目前的controlnet控制手部和躯体姿势效果较好，但是手部动作要清晰可见，如果手部被遮挡或动作不明显不清晰，会导致控制效果下降。

4. 不要拿拿着物品的图来控制姿势，因为姿势图无法输出物品，而你提示词再加入物品，会导致物品随机生成。
